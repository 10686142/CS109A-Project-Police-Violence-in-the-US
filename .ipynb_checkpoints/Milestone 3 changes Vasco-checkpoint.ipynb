{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can you update this part of the main file to this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information about the datasets\n",
    "\n",
    "#### Washingpost dataset\n",
    "In 2015, The Post began tracking more than a dozen details about each killing — including the race of the deceased, the circumstances of the shooting, whether the person was armed and whether the person was experiencing a mental-health crisis — by culling local news reports, law enforcement websites and social media, and by monitoring independent databases such as Killed by Police and Fatal Encounters. The Post conducted additional reporting in many cases.\n",
    "\n",
    "#### Mapping Police Violence\n",
    "This information has been meticulously sourced from the three largest, most comprehensive and impartial crowdsourced databases on police killings in the country: FatalEncounters.org, the U.S. Police Shootings Database and KilledbyPolice.net. \n",
    "\n",
    "#### Important notes\n",
    "The mapping police violence dataset has more predictors and data entries which could still be of interest for this analysis, but it also has some NaN values and incomplete data. Washington Post sources their data from the mapping police violence and that datasets that mapping police violence also uses, but they cleaned the data more completely making it more useful for many required comparisons in this investigation. Therefore, the best data source to answer the proposed analysis questions is the Washington Post dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can you add these cells somewhere at the stard of the wsp part?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional population data\n",
    "We also believe that population density and classifications that could be made from this information like rural or urban cities could be important in our models. So we scraped the List of United States cities by population wikipedia page to retrieve the densities of the most densely populated cities. This wikipedia page sourced their information from the United States Census Bureau. We matched the city names from our the listed incidents of the Washington Post dataset to the population density from the scraped data and added a new Population density km2 column to our dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading our scraped city population data\n",
    "\n",
    "We scraped a wikipedia entry with population densities from cities with our 100k residents. We use this to determine if our incidents happened in urban or dense areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filename of the scraped wiki table data\n",
    "wiki_csv_output = \"wiki-table-population.csv\"\n",
    "\n",
    "# https://github.com/washingtonpost/data-police-shootings\n",
    "df_cities = pd.read_csv(f'{data_dir}/{wiki_csv_output}')  \n",
    "\n",
    "# Output \n",
    "display(df_cities.head())\n",
    "display(df_cities.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding population density of large cities\n",
    "\n",
    "About half of the cities are not matched with the population density set because:\n",
    "- The population dataset is from each city with a population over 100k residents, so small cities won't match\n",
    "- Some city names are defined differently in both dataset. I already added some fixes in the data collection part but we can probably still match some other 100k population cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the wsp cities for cross referencing\n",
    "cities_to_match = df_cities['city'].tolist()\n",
    "\n",
    "# Keeping track\n",
    "city_densities = []\n",
    "city_fail_amount = 0\n",
    "\n",
    "for i, city in df_wsp['city'].iteritems():\n",
    "    if city not in  cities_to_match:\n",
    "        city_densities.append(0)\n",
    "#         print(f\"ERROR Match city -> {city}\")\n",
    "        city_fail_amount += 1\n",
    "    else:\n",
    "        pop_row = df_cities.loc[df_cities['city'] == city]\n",
    "#         print(type(pop_row))\n",
    "        city_densities.append(pop_row['pop_dens_2016_km'].iloc[0])\n",
    "#         print(f\"Matched city -> {city}\")\n",
    "        continue\n",
    "\n",
    "print(f\"Did not match {city_fail_amount} (+100k residents) cities\")\n",
    "\n",
    "# Create a column from the list\n",
    "df_wsp['pop_dens_2016_km'] = city_densities\n",
    "df_wsp.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the 'urban' classifier\n",
    "\n",
    "We used a simple metric of a population km2 density above 2900 to be an 'urban' city. I got the idea from:\n",
    "https://medium.com/codait/got-zip-code-data-prep-it-for-analytics-7022b47652d9\n",
    "\n",
    "But we can probably make the classification more accurate or perhaps add more classes or create our own density based classes which could perhaps by good predictors for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add population density\n",
    "# Classification from: https://medium.com/codait/got-zip-code-data-prep-it-for-analytics-7022b47652d9\n",
    "\n",
    "# Simplified to boolean\n",
    "df_wsp['urban'] = np.where(df_wsp['pop_dens_2016_km'] >= 2900, 'yes', 'no')\n",
    "df_wsp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the next part is optional, because the visualization itself is not that greate and you need 2 librarie installed to run this\n",
    "\n",
    "##### To install them run these 2 commands:\n",
    "- pip install geopandas\n",
    "- pip install descartes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a mapped visualzation\n",
    "\n",
    "\n",
    "#### The state of our visualizations\n",
    "We create a US map so we can better inspect where these indicents occur. We only just started working with geopandas and all it's required depencies, so our visualizations are not that great yet. We do however plan to make more specific state based visualizations, which we can use to inspect where incidents most occur and determine is a precictor like 'urban' can be of importance.\n",
    "\n",
    "#### Imputing missing coordinates\n",
    "We miss about 600 latitude and longituge points from are dataset, which you can see in this notebook. So for now we did not use the rows with missing coordinates. We do however plan to impute these coordinates by the coordinates we can cross reference from each corresponding city. This means we'll have can reduce these 600 missing coordinates to 0, since no incident has a missing value for city. We plan to use [geopy](https://geopy.readthedocs.io/en/stable/) for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Right way to install geopands in existing conda env\n",
    "# Had to do it with pip, alse needed pip install descartes\n",
    "# https://stackoverflow.com/questions/34427788/how-to-successfully-install-pyproj-and-geopandas\n",
    "import geopandas\n",
    "\n",
    "# Create a new df withouth the rows that miss the required rows for mapping\n",
    "df_wsp_coord_cleared = df_wsp[df_wsp.longitude.notnull()]\n",
    "\n",
    "# A GeoDataFrame needs a shapely object. We use geopandas points_from_xy()\n",
    "# to transform Longitude and Latitude into a list of shapely.Point\n",
    "gdf = geopandas.GeoDataFrame(df_wsp_coord_cleared,\n",
    "                             geometry=geopandas.points_from_xy(df_wsp_coord_cleared.longitude,\n",
    "                                                               df_wsp_coord_cleared.latitude))\n",
    "# Output the results for inspection\n",
    "display(gdf.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Docs:https://geopandas.org/gallery/create_geopandas_from_pandas.html\n",
    "# Other source:https://jcutrer.com/python/learn-geopandas-plotting-usmaps\n",
    "# https://towardsdatascience.com/geopandas-101-plot-any-data-with-a-latitude-and-longitude-on-a-map-98e01944b972\n",
    "# Get the dataset for the naturalearth_lowres\n",
    "world = geopandas.read_file(geopandas.datasets.get_path('naturalearth_lowres'))\n",
    "\n",
    "# We restrict to US.\n",
    "ax = world[world.name == \"United States of America\"].plot(\n",
    "    color='white', edgecolor='black')\n",
    "\n",
    "# We can now plot our ``GeoDataFrame``.\n",
    "gdf.plot(ax=ax, color='red')\n",
    "\n",
    "# Output our first simple US map\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can you remove these 2 cells?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding race values\n",
    "We do miss about 600 entries of race.\n",
    "\n",
    "This is the [original classification](https://github.com/washingtonpost/data-police-shootings):\n",
    "race:\n",
    "\n",
    "`race`:\n",
    "- `W`: White, non-Hispanic\n",
    "- `B`: Black, non-Hispanic\n",
    "- `A`: Asian\n",
    "- `N`: Native American\n",
    "- `H`: Hispanic\n",
    "- `O`: Other\n",
    "- `None`: unknown\n",
    "\n",
    "We want add weights the incident based on the actual amount of each race that lives in the US.\n",
    "\n",
    "Used data [from wikipedia](https://en.wikipedia.org/wiki/Race_and_ethnicity_in_the_United_States#Racial_categories):\n",
    "- `W`: 0.601\n",
    "- `B`: 0.134\n",
    "- `A`: 0.059\n",
    "- `N`: 0.013\n",
    "- `H`: 0.185\n",
    "- `O`: 0.008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check our different values \n",
    "display(df2['race'].unique())\n",
    "\n",
    "# Add the weighted race values from wikipedia \n",
    "race_ratios = {\n",
    "    'W': 0.601,\n",
    "    'B': 0.134,\n",
    "    'A': 0.059,\n",
    "    'N': 0.013,\n",
    "    'H': 0.185,\n",
    "    'O': 0.008,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TODO:Impute the unkown values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
